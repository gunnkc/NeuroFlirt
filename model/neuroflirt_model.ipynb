{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing & Feature Engineering"
      ],
      "metadata": {
        "id": "norvFNGgL86j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_path = None # input file_path of Kaggle database-for-emotion-recognition-system dataset\n",
        "# https://www.kaggle.com/datasets/sigfest/database-for-emotion-recognition-system-gameemo\n",
        "\n",
        "gameemo_df = pd.read_csv(file_path)\n",
        "processed_df = pd.DataFrame()\n",
        "\n",
        "processed_df['TP9'] = (gameemo_df['T7'] + gameemo_df['P7']) / 2\n",
        "processed_df['TP10'] = (gameemo_df['T8'] + gameemo_df['P8']) / 2\n",
        "processed_df['AF7'] = (gameemo_df['AF3'] + gameemo_df['F7'] + gameemo_df['FC5'] + gameemo_df['F3']) / 4\n",
        "processed_df['AF8'] = (gameemo_df['AF4'] + gameemo_df['F8'] + gameemo_df['FC6'] + gameemo_df['F4']) / 4"
      ],
      "metadata": {
        "id": "0sOeV-6CMOro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePVBFZ4QLXZ2"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# the most basic training data that includes features from full windows after processing\n",
        "# with batch size input\n",
        "def generate_summary_stats_training_data(df, batch_size):\n",
        "    # df : unprocessed dataframe (No batch processing)\n",
        "    # batch_size : indicate processing size (# of seconds)\n",
        "    # training columns without creating covariance matrices\n",
        "    train_cols = ['mean_tp9', 'mean_tp10', 'mean_af7', 'mean_af8',\n",
        "                     'std_tp9', 'std_tp10', 'std_af7', 'std_af8',\n",
        "                     'vari_tp9', 'vari_tp10', 'vari_af7', 'vari_af8',\n",
        "                     'max_tp9', 'max_tp10', 'max_af7', 'max_af8',\n",
        "                     'min_tp9', 'min_tp10', 'min_af7', 'min_af8',\n",
        "                     'skew_tp9', 'skew_tp10', 'skew_af7', 'skew_af8',\n",
        "                     'kurt_tp9', 'kurt_tp10', 'kurt_af7', 'kurt_af8']\n",
        "    train_cols_dict = dict(zip(train_cols, [[] for i in range(len(train_cols))]))\n",
        "\n",
        "\n",
        "\n",
        "    for start in range(0, df.shape[0], batch_size):\n",
        "        end = start + batch_size\n",
        "        if end > len(df):\n",
        "            end = len(df)\n",
        "        batch = df[start:end]\n",
        "\n",
        "        # add mean summary stats on all 4 channels\n",
        "        train_cols_dict['mean_tp9'].append(batch['TP9'].mean())\n",
        "        train_cols_dict['mean_tp10'].append(batch['TP10'].mean())\n",
        "        train_cols_dict['mean_af7'].append(batch['AF7'].mean())\n",
        "        train_cols_dict['mean_af8'].append(batch['AF8'].mean())\n",
        "\n",
        "        # add std summary stats on all 4 channels\n",
        "        train_cols_dict['std_tp9'].append(np.std(batch['TP9'], ddof=1))\n",
        "        train_cols_dict['std_tp10'].append(np.std(batch['TP10'], ddof=1))\n",
        "        train_cols_dict['std_af7'].append(np.std(batch['AF7'], ddof=1))\n",
        "        train_cols_dict['std_af8'].append(np.std(batch['AF8'], ddof=1))\n",
        "\n",
        "        # add variance\n",
        "        train_cols_dict['vari_tp9'].append(np.var(batch['TP9'], ddof=1))\n",
        "        train_cols_dict['vari_tp10'].append(np.var(batch['TP10'], ddof=1))\n",
        "        train_cols_dict['vari_af7'].append(np.var(batch['AF7'], ddof=1))\n",
        "        train_cols_dict['vari_af8'].append(np.var(batch['AF8'], ddof=1))\n",
        "\n",
        "        # add min\n",
        "        train_cols_dict['min_tp9'].append(batch['TP9'].min())\n",
        "        train_cols_dict['min_tp10'].append(batch['TP10'].min())\n",
        "        train_cols_dict['min_af7'].append(batch['AF7'].min())\n",
        "        train_cols_dict['min_af8'].append(batch['AF8'].min())\n",
        "\n",
        "        # add max\n",
        "        train_cols_dict['max_tp9'].append(batch['TP9'].max())\n",
        "        train_cols_dict['max_tp10'].append(batch['TP10'].max())\n",
        "        train_cols_dict['max_af7'].append(batch['AF7'].max())\n",
        "        train_cols_dict['max_af8'].append(batch['AF8'].max())\n",
        "\n",
        "        # add skew\n",
        "        train_cols_dict['skew_tp9'].append(skew(np.array(batch['TP9'])))\n",
        "        train_cols_dict['skew_tp10'].append(skew(np.array(batch['TP10'])))\n",
        "        train_cols_dict['skew_af7'].append(skew(np.array(batch['AF7'])))\n",
        "        train_cols_dict['skew_af8'].append(skew(np.array(batch['AF8'])))\n",
        "\n",
        "        # add kurtosis\n",
        "        train_cols_dict['kurt_tp9'].append(kurtosis(np.array(batch['TP9'])))\n",
        "        train_cols_dict['kurt_tp10'].append(kurtosis(np.array(batch['TP10'])))\n",
        "        train_cols_dict['kurt_af7'].append(kurtosis(np.array(batch['AF7'])))\n",
        "        train_cols_dict['kurt_af8'].append(kurtosis(np.array(batch['AF8'])))\n",
        "\n",
        "    output_training_df = pd.DataFrame(train_cols_dict)\n",
        "    return output_training_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection using 5-Fold Cross Validation"
      ],
      "metadata": {
        "id": "qhep1h_IMs4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "def feature_selection(df):\n",
        "  # df : processed sliding window training data and corresponding output labels\n",
        "\n",
        "  k_range = range(1, 29) # num of features = 28\n",
        "\n",
        "  # Initialize variables to keep track of the best feature set and its RMSE\n",
        "  best_feature_set = None\n",
        "  best_rmse = float('inf')\n",
        "\n",
        "  # create train test split\n",
        "  pred = 'Boring'\n",
        "  data = df.drop(columns=['Valence', 'Boring', 'Arousal', 'Funny', 'Calm', 'Satisfaction', 'Horrible'])\n",
        "  label = df[pred]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
        "\n",
        "  # normalize/ sclae training data\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(X_train)\n",
        "  X_train_scaled = scaler.transform(X_train)\n",
        "\n",
        "\n",
        "  # Iterate over different values of k for feature selection\n",
        "  for k in k_range:\n",
        "\n",
        "      # Perform feature selection\n",
        "      selector = SelectKBest(score_func=f_regression, k=k)\n",
        "      X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "\n",
        "      # Initialize LGBMRegressor\n",
        "      regressor = LGBMRegressor()\n",
        "\n",
        "      # Evaluate accuracy using cross-validation\n",
        "      cv_scores = cross_val_score(regressor, X_train_selected, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "      rmse_scores = np.sqrt(-cv_scores)\n",
        "      avg_rmse = np.mean(rmse_scores)\n",
        "      print(\"current r MSE: \", avg_rmse)\n",
        "      print(\"number of features\", k)\n",
        "\n",
        "      # Update the best feature set and its RMSE if necessary\n",
        "      if avg_rmse < best_rmse:\n",
        "          best_rmse = avg_rmse\n",
        "          best_feature_set = selector.get_support(indices=True)\n",
        "\n",
        "  print(\"best feature set: \", best_feature_set)\n",
        "  print(\"best r MSE: \", best_rmse)"
      ],
      "metadata": {
        "id": "5bgv5aixMsjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Grid Search for Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "VEJyeepFOUn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def optimize_hyperparameters(df):\n",
        "  # df : processed sliding window training data and corresponding output labels\n",
        "\n",
        "\n",
        "  # create train test split\n",
        "  pred = 'Arousal'\n",
        "  data = df.drop(columns=['Valence', 'Boring', 'Arousal', 'Funny', 'Calm', 'Satisfaction', 'Horrible'])\n",
        "  print(\"feature columns: \", data.columns)\n",
        "  label = df[pred]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
        "\n",
        "  # normalize/ sclae training data\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(X_train)\n",
        "  X_train_scaled = scaler.transform(X_train)\n",
        "\n",
        "  # Define the parameter grid\n",
        "  param_grid = {\n",
        "      'num_leaves': [20, 30, 40],\n",
        "      'learning_rate': [0.05, 0.1, 0.2],\n",
        "      'n_estimators': [50, 100, 200]\n",
        "  }\n",
        "\n",
        "  # Initialize the LGBMRegressor\n",
        "  lgbm_regressor = LGBMRegressor()\n",
        "\n",
        "  # Initialize GridSearchCV\n",
        "  grid_search = GridSearchCV(lgbm_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "  # Perform grid search\n",
        "  grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Get the best hyperparameters\n",
        "  best_params = grid_search.best_params_\n",
        "  print(\"Best Hyperparameters:\", best_params)"
      ],
      "metadata": {
        "id": "OVFbNEq8OYRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run lightGBM Model Using Selected Features and Optimized Hyperparameters"
      ],
      "metadata": {
        "id": "iAOPY5grPJsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "def run_lgbm_regre_model(df):\n",
        "    # df : processed dataframe by batch size. Features include full time window summary stats, etc.\n",
        "\n",
        "    pred = ['Valence', 'Boring', 'Arousal']\n",
        "\n",
        "    data = df.drop(columns=['Valence', 'Boring', 'Arousal', 'Funny', 'Calm', 'Satisfaction', 'Horrible'])\n",
        "    label = df[pred]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # define best hyperparameters as result of GridSearch\n",
        "    hyperparameters = {\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 200,\n",
        "    'num_leaves': 30\n",
        "    }\n",
        "    clf = MultiOutputRegressor(LGBMRegressor(** hyperparameters)).fit(X_train_scaled, y_train)\n",
        "    pred = clf.predict(X_test_scaled)\n",
        "    print(pred)\n",
        "    return clf"
      ],
      "metadata": {
        "id": "E3iJWhX7PJ7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}